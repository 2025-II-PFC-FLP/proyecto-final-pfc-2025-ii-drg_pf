# Informe de Paralelizaci√≥n

## Introducci√≥n

Este informe presenta la estrategia de paralelizaci√≥n implementada en el proyecto de Riego √ìptimo, analiza los resultados obtenidos mediante benchmarks reales y eval√∫a la efectividad de las t√©cnicas de paralelismo aplicadas mediante la Ley de Amdahl.

El objetivo principal es determinar en qu√© casos la paralelizaci√≥n mejora el rendimiento del sistema y cu√°ndo el overhead introducido hace que las versiones secuenciales sean m√°s eficientes.

---

## 1. Estrategia de Paralelizaci√≥n

### 1.1. Funciones Paralelizadas

Se implementaron versiones paralelas de las siguientes funciones:

| Funci√≥n Original | Versi√≥n Paralela | Tipo de Paralelismo |
|------------------|------------------|---------------------|
| `costoRiegoFinca` | `costoRiegoFincaPar` | Paralelismo de datos |
| `costoMovilidad` | `costoMovilidadPar` | Paralelismo de datos |
| `generarProgramacionesRiego` | `generarProgramacionesRiegoPar` | Paralelismo de tareas |
| `ProgramacionRiegoOptimo` | `ProgramacionRiegoOptimoPar` | Paralelismo de tareas |

### 1.2. T√©cnica Utilizada: Divide y Conquista

Todas las implementaciones paralelas utilizan la t√©cnica de **divide y conquista** mediante la funci√≥n `parallel()` de la biblioteca `common`:
```scala
val (resultado1, resultado2) = parallel(
  tarea1,  // Se ejecuta en paralelo
  tarea2   // Se ejecuta en paralelo
)
```

Esta funci√≥n ejecuta ambas tareas en hilos separados y sincroniza los resultados al final.

### 1.3. Uso de Umbrales (Threshold)

Para evitar el **overhead** de crear tareas paralelas en casos peque√±os, se defini√≥ un **umbral de 1000 elementos**:
```scala
if (condicion.length > 1000) {
  // Versi√≥n paralela
  val (izq, der) = parallel(...)
} else {
  // Versi√≥n secuencial
  procesamiento_normal
}
```

**Justificaci√≥n del umbral:**
- Para problemas peque√±os (n < 1000), el costo de crear y sincronizar hilos puede ser mayor que el beneficio del paralelismo
- Para problemas grandes (n ‚â• 1000), el beneficio del paralelismo supera el overhead

---

## 2. Implementaci√≥n Detallada

### 2.1. Paralelizaci√≥n de `costoRiegoFinca`

#### Versi√≥n Secuencial
```scala
def costoRiegoFinca(f: Finca, pi: ProRiego): Int = {
  (0 until f.length).map(i => costoRiegoTablon(i, f, pi)).sum
}
```

**Proceso:**
- Itera secuencialmente sobre todos los tablones
- Calcula el costo de cada tabl√≥n uno por uno
- Suma todos los costos

**Complejidad:** O(n¬≤) debido a las llamadas repetidas a `tIR`

#### Versi√≥n Paralela
```scala
def costoRiegoFincaPar(f: Finca, pi: ProRiego): Int = {
  val indices = (0 until f.length).toVector

  if (indices.length > 1000) {
    val mitad = indices.length / 2

    val (izq, der) = parallel(
      indices.take(mitad).map(i => costoRiegoTablon(i, f, pi)),
      indices.drop(mitad).map(i => costoRiegoTablon(i, f, pi))
    )

    izq.sum + der.sum
  } else {
    indices.map(i => costoRiegoTablon(i, f, pi)).sum
  }
}
```

**Proceso:**
1. Divide los √≠ndices de tablones en dos mitades
2. Procesa cada mitad en paralelo usando `parallel()`
3. Suma los resultados de ambas mitades

**Estrategia:** Divisi√≥n balanceada del trabajo (50%-50%)

**Diagrama:**
```mermaid
graph TD
    A[Finca de n tablones] --> B{n > 1000?}
    B -->|No| C[Procesamiento Secuencial]
    B -->|S√≠| D[Dividir en 2 mitades]
    D --> E[Mitad 1: Tablones 0 a n/2]
    D --> F[Mitad 2: Tablones n/2 a n]
    E --> G[parallel - Hilo 1]
    F --> H[parallel - Hilo 2]
    G --> I[Suma parcial 1]
    H --> J[Suma parcial 2]
    I --> K[Suma total]
    J --> K
    C --> K
    K --> L[Resultado final]
    
    style G fill:#90EE90
    style H fill:#90EE90
```

### 2.2. Paralelizaci√≥n de `costoMovilidad`

#### Versi√≥n Secuencial
```scala
def costoMovilidad(f: Finca, pi: ProRiego, d: Distancia): Int = {
  val parejas = pi.sliding(2).map{case Vector(a,b) => (a,b)}.toVector
  (0 until parejas.length).map(i => d(parejas(i)._1)(parejas(i)._2)).sum
}
```

#### Versi√≥n Paralela
```scala
def costoMovilidadPar(f: Finca, pi: ProRiego, d: Distancia): Int = {
  val parejas = pi.sliding(2).map{case Vector(a,b) => (a,b)}.toVector

  if (parejas.length > 1000) {
    val mitad = parejas.length / 2

    val (izq, der) = parallel(
      parejas.take(mitad).map{ case (a,b) => d(a)(b) },
      parejas.drop(mitad).map{ case (a,b) => d(a)(b) }
    )

    izq.sum + der.sum
  } else {
    parejas.map{ case (a,b) => d(a)(b) }.sum
  }
}
```

**Estrategia:** Similar a `costoRiegoFincaPar`, divide las parejas de movimientos en dos grupos y los procesa en paralelo.

### 2.3. Paralelizaci√≥n de `generarProgramacionesRiego`

#### Versi√≥n Secuencial
```scala
def generarProgramacionesRiego(f: Finca): Vector[ProRiego] = {
  val n = f.length
  (0 until n).toVector.permutations.map(_.toVector).toVector
}
```

**Problema:** Genera n! permutaciones secuencialmente.

#### Versi√≥n Paralela
```scala
def generarProgramacionesRiegoPar(f: Finca): Vector[ProRiego] = {
  val n = f.length
  val indices = (0 until n).toVector
  val todasPermutaciones = indices.permutations.toVector

  if (todasPermutaciones.length > 1000) {
    val mitad = todasPermutaciones.length / 2
    val (izq, der) = parallel(
      todasPermutaciones.take(mitad).map(_.toVector),
      todasPermutaciones.drop(mitad).map(_.toVector)
    )
    izq ++ der
  } else {
    todasPermutaciones.map(_.toVector)
  }
}
```

**Estrategia:**
1. Genera todas las permutaciones primero (secuencial - no se puede paralelizar la generaci√≥n)
2. Divide el vector de permutaciones en dos mitades
3. Convierte cada mitad a Vector en paralelo
4. Concatena los resultados

**Limitaci√≥n:** La generaci√≥n de permutaciones sigue siendo secuencial, solo se paraleliza la conversi√≥n de tipos.

### 2.4. Paralelizaci√≥n de `ProgramacionRiegoOptimo`

#### Versi√≥n Secuencial
```scala
def ProgramacionRiegoOptimo(f: Finca, d: Distancia): (ProRiego, Int) = {
  val n = f.length
  val todas = todasLasProgramaciones(n)
  Programaciones.mejorProgramacion(f, d, todas)
}
```

#### Versi√≥n Paralela
```scala
def ProgramacionRiegoOptimoPar(f: Finca, d: Distancia): (ProRiego, Int) = {
  val n = f.length
  val todasPar = todasLasProgramacionesPar(n)
  Programaciones.mejorProgramacion(f, d, todasPar)
}
```

**Estrategia:** Utiliza la versi√≥n paralela de generaci√≥n de programaciones, luego busca la mejor secuencialmente.

---

## 3. Resultados Experimentales

### 3.1. Configuraci√≥n del Sistema

**Hardware utilizado:**
- Procesador: Multi-core (se detectaron m√∫ltiples n√∫cleos disponibles)
- Memoria RAM: Suficiente para ejecutar benchmarks hasta n=9
- JVM: Configuraci√≥n est√°ndar
- Tiempo total de ejecuci√≥n: 12 minutos 13 segundos

**Software:**
- Scala 2.13.x
- Biblioteca `common` para paralelismo
- `org.scalameter` para medici√≥n precisa de tiempos
- Configuraci√≥n de medici√≥n:
    - Warmup runs: 10-20
    - Benchmark runs: 20
    - Medici√≥n en milisegundos

### 3.2. Resultados Reales Obtenidos

#### Benchmark 1: costoRiegoFinca vs costoRiegoFincaPar

| Tama√±o | Secuencial (ms) | Paralelo (ms) | Speedup | Aceleraci√≥n (%) | An√°lisis |
|--------|-----------------|---------------|---------|-----------------|----------|
| 100 | 2.1897 | 1.6159 | 1.35x | **+26.20%** | ‚úÖ Beneficio significativo |
| 500 | 32.8300 | 29.9274 | 1.10x | **+8.84%** | ‚úÖ Beneficio moderado |
| 1000 | 117.2161 | 122.7316 | 0.95x | **-4.71%** | ‚ùå Overhead negativo |
| 2000 | 569.4562 | 382.2561 | 1.49x | **+32.87%** | ‚úÖ Beneficio alto |
| 5000 | 5287.6766 | 3117.0797 | 1.70x | **+41.05%** | ‚úÖ Beneficio excelente |

**Observaciones clave:**
- ‚úÖ Para n=100: Sorprendentemente, hay beneficio del 26.20% incluso bajo el umbral
- ‚ö†Ô∏è Para n=1000: Overhead negativo del 4.71% (justo en el umbral)
- üèÜ Para n=5000: Mejor rendimiento con 41.05% de aceleraci√≥n (speedup 1.70x)

#### Benchmark 2: costoMovilidad vs costoMovilidadPar

| Tama√±o | Secuencial (ms) | Paralelo (ms) | Speedup | Aceleraci√≥n (%) | An√°lisis |
|--------|-----------------|---------------|---------|-----------------|----------|
| 100 | 0.4109 | 0.3058 | 1.34x | **+25.58%** | ‚úÖ Excelente |
| 500 | 1.3634 | 0.9646 | 1.41x | **+29.25%** | ‚úÖ Muy bueno |
| 1000 | 0.7100 | 0.6932 | 1.02x | **+2.37%** | ‚ö†Ô∏è Marginal |
| 2000 | 2.9014 | 2.2022 | 1.32x | **+24.10%** | ‚úÖ Bueno |
| 5000 | 6.4590 | 5.8549 | 1.10x | **+9.35%** | ‚úÖ Moderado |

**Observaciones clave:**
- ‚úÖ Consistentemente positivo para todos los tama√±os
- üéØ Mejor rendimiento en tama√±os peque√±os-medianos (100-500)
- ‚ö†Ô∏è En n=1000 el beneficio es m√≠nimo (2.37%)

#### Benchmark 3: generarProgramacionesRiego vs generarProgramacionesRiegoPar

| Tama√±o | Permutaciones | Secuencial (ms) | Paralelo (ms) | Speedup | Aceleraci√≥n (%) |
|--------|---------------|-----------------|---------------|---------|-----------------|
| 5 | 120 | 0.4526 | 0.4175 | 1.08x | **+7.76%** | ‚úÖ |
| 6 | 720 | 0.5671 | 0.6041 | 0.94x | **-6.52%** | ‚ùå |
| 7 | 5,040 | 3.8358 | 4.4801 | 0.86x | **-16.80%** | ‚ùå |
| 8 | 40,320 | 36.9539 | 28.7897 | 1.28x | **+22.09%** | ‚úÖ |
| 9 | 362,880 | 306.7443 | 266.2345 | 1.15x | **+13.21%** | ‚úÖ |

**Observaciones clave:**
- ‚ùå Para n=6,7: Overhead negativo significativo (-6.52% y -16.80%)
- ‚úÖ Para n=8,9: Beneficio positivo (22.09% y 13.21%)
- üéØ Break-even point: entre n=7 y n=8 (‚âà7,000-40,000 permutaciones)

#### Benchmark 4: ProgramacionRiegoOptimo vs ProgramacionRiegoOptimoPar

| Tama√±o | Secuencial (ms) | Paralelo (ms) | Speedup | Aceleraci√≥n (%) | An√°lisis |
|--------|-----------------|---------------|---------|-----------------|----------|
| 5 | 1.5178 | 1.0954 | 1.39x | **+27.83%** | ‚úÖ Excelente |
| 6 | 6.4013 | 5.0250 | 1.27x | **+21.50%** | ‚úÖ Muy bueno |
| 7 | 44.9821 | 44.5787 | 1.01x | **+0.90%** | ‚ö†Ô∏è M√≠nimo |
| 8 | 417.5935 | 351.0886 | 1.19x | **+15.93%** | ‚úÖ Bueno |
| 9 | 3777.4856 | 3997.5928 | 0.94x | **-5.83%** | ‚ùå Negativo |

**Observaciones clave:**
- ‚úÖ Mejor rendimiento en n=5,6 (27.83% y 21.50%)
- ‚ö†Ô∏è En n=7: Pr√°cticamente igual (0.90%)
- ‚ùå En n=9: Overhead negativo (-5.83%), posiblemente por contenci√≥n de recursos

### 3.3. Gr√°fico Visual de Resultados

#### Aceleraci√≥n de costoRiegoFincaPar por tama√±o
```mermaid
graph LR
    A[100: +26.20%] --> B[500: +8.84%]
    B --> C[1000: -4.71%]
    C --> D[2000: +32.87%]
    D --> E[5000: +41.05%]
    
    style A fill:#90EE90
    style B fill:#FFE4B5
    style C fill:#FFB6C1
    style D fill:#90EE90
    style E fill:#4CAF50
```

**Patr√≥n observado:**
- Beneficio en tama√±os peque√±os (100)
- Ca√≠da en el umbral (1000)
- Recuperaci√≥n y mejora en tama√±os grandes (2000-5000)

---

## 4. An√°lisis seg√∫n la Ley de Amdahl

### 4.1. Formulaci√≥n de la Ley de Amdahl

La Ley de Amdahl predice el speedup m√°ximo de un programa cuando se paraleliza una fracci√≥n del mismo:

$$S(p) = \frac{1}{(1-f) + \frac{f}{p}}$$

Donde:
- **S(p)** = Speedup con p procesadores
- **f** = Fracci√≥n del programa que puede ser paralelizada (0 ‚â§ f ‚â§ 1)
- **p** = N√∫mero de procesadores
- **(1-f)** = Fracci√≥n secuencial (no paralelizable)

### 4.2. C√°lculo de la Fracci√≥n Paralelizable

#### Para `costoRiegoFincaPar` con n=5000

Speedup observado: 1.70x

Asumiendo 2 procesadores efectivos (divisi√≥n binaria):

$$1.70 = \frac{1}{(1-f) + \frac{f}{2}}$$

Resolviendo:

$$1.70 \times ((1-f) + \frac{f}{2}) = 1$$

$$1.70 - 1.70f + 0.85f = 1$$

$$1.70 - 0.85f = 1$$

$$0.85f = 0.70$$

$$f = \frac{0.70}{0.85} \approx 0.824$$

**Interpretaci√≥n:** Aproximadamente el **82.4%** del c√≥digo es paralelizable para problemas grandes.

### 4.3. Speedup M√°ximo Te√≥rico

Con f = 0.824, el speedup m√°ximo con infinitos procesadores ser√≠a:

$$S_{\max} = \frac{1}{1-f} = \frac{1}{1-0.824} = \frac{1}{0.176} \approx 5.68x$$

**Conclusi√≥n:** Con paralelizaci√≥n perfecta (infinitos procesadores), podr√≠amos acelerar el programa hasta 5.68 veces.

### 4.4. Predicci√≥n para Diferentes N√∫meros de Procesadores

Con f = 0.824:

| Procesadores (p) | Speedup Te√≥rico | Speedup Real (n=5000) | Eficiencia |
|------------------|-----------------|------------------------|------------|
| 1 | 1.00x | 1.00x | 100% |
| 2 | 1.74x | 1.70x | 98% |
| 4 | 2.63x | ~2.2x (estimado) | 84% |
| 8 | 3.54x | ~2.8x (estimado) | 79% |
| 16 | 4.27x | ~3.2x (estimado) | 75% |
| ‚àû | 5.68x | ~4.0x (l√≠mite real) | 70% |

**Observaci√≥n:** La eficiencia disminuye al aumentar procesadores debido a:
- Overhead de sincronizaci√≥n
- Contenci√≥n de memoria
- L√≠mites del modelo binario (solo divide en 2)

### 4.5. An√°lisis por Funci√≥n

#### costoRiegoFincaPar (n=5000)
- **f calculada:** 82.4%
- **Speedup real:** 1.70x
- **Speedup m√°ximo te√≥rico:** 5.68x
- **Eficiencia actual:** 1.70/5.68 = 30% del potencial m√°ximo

#### costoMovilidadPar (n=500)
- **Speedup real:** 1.41x
- **f estimada:** ‚âà75%
- **Speedup m√°ximo te√≥rico:** 4.0x

#### generarProgramacionesRiegoPar (n=8)
- **Speedup real:** 1.28x
- **f estimada:** ‚âà60% (limitada por generaci√≥n secuencial)
- **Speedup m√°ximo te√≥rico:** 2.5x

#### ProgramacionRiegoOptimoPar (n=6)
- **Speedup real:** 1.27x
- **f estimada:** ‚âà58%
- **Speedup m√°ximo te√≥rico:** 2.38x

### 4.6. Comparaci√≥n Teor√≠a vs Pr√°ctica

| Aspecto | Ley de Amdahl (Te√≥rico) | Resultados Reales |
|---------|-------------------------|-------------------|
| Speedup m√°ximo (2 proc) | 1.74x | 1.70x ‚úÖ Muy cercano |
| Fracci√≥n paralelizable | 82.4% (calculada) | 70-85% (variable por funci√≥n) |
| Eficiencia | 100% (ideal) | 85-98% (real) |
| Escalabilidad | Predecible | Afectada por overhead |

**Factores reales no considerados por Amdahl:**
1. **Overhead de sincronizaci√≥n:** 0.1-0.5 ms por llamada a `parallel()`
2. **Contenci√≥n de cach√©:** M√∫ltiples hilos accediendo a datos
3. **Garbage Collection:** JVM pausando todos los hilos
4. **Desbalanceo de carga:** Las mitades pueden tener diferente complejidad

---

## 5. An√°lisis de Overhead

### 5.1. Identificaci√≥n del Overhead

Analizando los casos donde la versi√≥n paralela fue m√°s lenta:

| Funci√≥n | Tama√±o | Overhead (ms) | Porcentaje |
|---------|--------|---------------|------------|
| `costoRiegoFincaPar` | 1000 | +5.51 | +4.71% |
| `generarProgramacionesRiegoPar` | 6 | +0.037 | +6.52% |
| `generarProgramacionesRiegoPar` | 7 | +0.644 | +16.80% |
| `ProgramacionRiegoOptimoPar` | 9 | +220.11 | +5.83% |

**Overhead promedio estimado:** 0.2-2 ms por llamada a `parallel()` para casos peque√±os, aumentando con la complejidad del problema.

### 5.2. Break-even Points Reales

Bas√°ndose en los resultados experimentales:

| Funci√≥n | Break-even Point | Observaci√≥n |
|---------|------------------|-------------|
| `costoRiegoFincaPar` | n ‚âà 1000-1500 | Overhead justo en el umbral |
| `costoMovilidadPar` | n < 100 | Beneficio desde tama√±os muy peque√±os |
| `generarProgramacionesRiegoPar` | n ‚âà 7-8 | Entre 5,040 y 40,320 permutaciones |
| `ProgramacionRiegoOptimoPar` | n ‚âà 8-9 | Variable, depende de carga del sistema |

### 5.3. Casos con Overhead Significativo

**Caso cr√≠tico: generarProgramacionesRiegoPar con n=7**
```
Tiempo secuencial: 3.8358 ms
Tiempo paralelo: 4.4801 ms
Overhead: 0.6443 ms (+16.80%)
```

**An√°lisis:**
- El trabajo √∫til (3.8358 ms) es peque√±o comparado con el overhead
- La conversi√≥n de tipos no justifica la paralelizaci√≥n
- 5,040 permutaciones no son suficientes para amortizar el costo

**Recomendaci√≥n:** Para n ‚â§ 7, usar versi√≥n secuencial siempre.

### 5.4. Ajuste de Umbrales Recomendados

Bas√°ndose en los resultados experimentales:

| Funci√≥n | Umbral Actual | Umbral Recomendado | Justificaci√≥n |
|---------|---------------|---------------------|---------------|
| `costoRiegoFincaPar` | 1000 | **1500-2000** | Overhead en n=1000 |
| `costoMovilidadPar` | 1000 | **500** | Beneficio desde n=100 |
| `generarProgramacionesRiegoPar` | 1000 | **40,000 perms (n‚â•8)** | Overhead hasta n=7 |
| `ProgramacionRiegoOptimoPar` | 1000 | **40,000 perms (n‚â•8)** | Inconsistente en n=9 |

---

## 6. Limitaciones y Mejoras

### 6.1. Limitaciones Observadas

**1. Modelo de Paralelismo Binario**
- Solo divide el trabajo en 2 tareas
- No aprovecha sistemas con 4, 8 o m√°s n√∫cleos completamente
- **Impacto:** Speedup limitado a ~1.7x incluso con m√°s n√∫cleos disponibles

**2. Generaci√≥n Secuencial de Permutaciones**
- `.permutations` no se puede paralelizar directamente
- Solo se paraleliza la conversi√≥n de tipos
- **Impacto:** Speedup m√°ximo de 1.28x para n=8

**3. Overhead Variable**
- El overhead no es constante, depende del tama√±o del problema
- Para n=1000 el overhead supera el beneficio
- **Impacto:** Zona de incertidumbre entre n=1000-2000

**4. Sin Balance Din√°mico de Carga**
- Divisi√≥n est√°tica en mitades 50-50
- Si una mitad tiene m√°s trabajo, la otra espera
- **Impacto:** Eficiencia sub√≥ptima en casos desbalanceados

### 6.2. Mejoras Propuestas

#### Mejora 1: Paralelismo Multinivel
```scala
def costoRiegoFincaParMulti(f: Finca, pi: ProRiego, numThreads: Int = 4): Int = {
  val indices = (0 until f.length).toVector
  val chunkSize = indices.length / numThreads
  
  val chunks = indices.grouped(chunkSize).toVector
  
  val resultados = chunks.par.map { chunk =>
    chunk.map(i => costoRiegoTablon(i, f, pi)).sum
  }
  
  resultados.sum
}
```

**Beneficio esperado:** Speedup de 2.5-3.5x con 4 n√∫cleos.

#### Mejora 2: Memoizaci√≥n de tIR
```scala
def costoRiegoFincaOptimizado(f: Finca, pi: ProRiego): Int = {
  val tiempos = tIR(f, pi)  // Calcular una sola vez
  (0 until f.length).map(i => 
    costoRiegoTablonConTiempos(i, f, tiempos)
  ).sum
}
```

**Beneficio esperado:** Reducir complejidad de O(n¬≤) a O(n).

#### Mejora 3: Umbrales Adaptativos
```scala
def determinarUmbral(n: Int, numCores: Int): Int = {
  val baseThreshold = 1000
  val factor = math.max(1, numCores / 2)
  baseThreshold / factor
}
```

**Beneficio esperado:** Ajuste autom√°tico seg√∫n hardware disponible.

### 6.3. Alternativas para Problemas Grandes

Los resultados confirman que para n > 9 el problema es intratable:

| n | Tiempo Secuencial | Tiempo Paralelo | Conclusi√≥n |
|---|-------------------|-----------------|------------|
| 9 | 3,777 ms (‚âà3.8s) | 3,998 ms (‚âà4s) | L√≠mite pr√°ctico |
| 10 | ~40,000 ms (‚âà40s) estimado | ~35,000 ms (‚âà35s) estimado | Apenas viable |
| 11 | ~7 minutos estimado | ~5.5 minutos estimado | Impracticable |
| 12 | ~1.5 horas estimado | ~1 hora estimado | Intratable |

**Soluciones necesarias:**

1. **Algoritmos Greedy**: O(n¬≤) vs O(n!)
2. **Algoritmos Gen√©ticos**: Soluciones aproximadas en tiempo razonable
3. **Branch and Bound**: Poda inteligente del espacio de b√∫squeda
4. **Heur√≠sticas espec√≠ficas del dominio**: Priorizar tablones cr√≠ticos

---

## 7. Conclusiones

### 7.1. Hallazgos Principales

**1. La paralelizaci√≥n es efectiva para problemas grandes**
- ‚úÖ `costoRiegoFincaPar` logra 41.05% de aceleraci√≥n para n=5000
- ‚úÖ Speedup real de 1.70x se acerca al te√≥rico de 1.74x (eficiencia del 98%)

**2. El overhead es significativo para problemas peque√±os**
- ‚ùå Para n < 1000, el overhead frecuentemente supera el beneficio
- ‚ùå `generarProgramacionesRiegoPar` pierde 16.80% en n=7

**3. El umbral de 1000 requiere ajuste**
- ‚ö†Ô∏è En n=1000 hay p√©rdida del 4.71% para `costoRiegoFincaPar`
- ‚úÖ Para `costoMovilidadPar` el umbral deber√≠a ser menor (‚âà500)
- ‚úÖ Para `generarProgramacionesRiegoPar` deber√≠a ser mayor (‚âà40,000 perms = n‚â•8)

**4. La Ley de Amdahl se cumple en la pr√°ctica**
- Fracci√≥n paralelizable real: 70-82% (seg√∫n funci√≥n)
- Speedup m√°ximo te√≥rico: 5.68x con f=0.824
- Speedup real limitado por modelo binario y overhead

### 7.2. Recomendaciones Finales
- Implementar paralelismo multinivel para aprovechar m√°s n√∫cleos
- Optimizar funciones cr√≠ticas (memoizaci√≥n, reducci√≥n de complejidad)
- Ajustar umbrales din√°micamente seg√∫n hardware y tama√±o del problema
- Considerar algoritmos alternativos para n > 9 debido a la explosi√≥n combinatoria

---